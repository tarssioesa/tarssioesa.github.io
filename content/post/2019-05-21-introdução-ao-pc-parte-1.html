---
title: 'Introdução ao PC: Parte 1.'
author: Tarssio Barreto
date: '2019-05-21'
slug: introdução-ao-pc-parte-1
categories: []
tags: []
keywords:
  - tech
---



<div style="text-align: justify">

<div id="pca" class="section level2">
<h2>PCA</h2>
<p>De forma muito simples, o PCA tem as seguintes objetivos:</p>
<ol style="list-style-type: lower-alpha">
<li><p>Reduzir a dimensão dos dados em análise, visando eliminar sobreposições, a partir da combinação linear das variáveis originais.</p></li>
<li><p>Geometricamente, o objetivo do PCA é rotacionar o eixo de um espaço com p dimensões para uma nova posição, na qual os PCs são ordenados pela variância e a covariância entre cada par dos novos eixos é 0.</p></li>
<li><p>Esta técnica é comumente utilizada para visualização dos dados, regressão (PCR, PLS) e estudo de padrões (inclusive em imagens e textos).</p></li>
</ol>
<p>O passo-a-passo para criar o PCA é:</p>
<ol style="list-style-type: decimal">
<li><p>Definir a matriz de dados;</p></li>
<li><p>Calcular o vetor médio dos dados;</p></li>
<li><p>Subtrair a média de todos os itens;</p></li>
<li><p>Calcular a matriz de covâriancia;</p></li>
<li><p>Calcular autovalores e autovetores</p></li>
</ol>
</div>
<div id="pacotes-que-serao-utilizados" class="section level2">
<h2>Pacotes que serão utilizados:</h2>
<pre class="r"><code>#install.packages(&quot;pacman&quot;)
library(pacman)


p_load(caret, tidyverse, factoextra, epubr, tm, lexiconPT, broom, tidytext, widyr, irlba, 
       Rtsne,plotly)</code></pre>
<div id="realizando-o-pca-e-analisando-os-resultados" class="section level3">
<h3>Realizando o PCA e analisando os resultados</h3>
<div id="carregando-os-dados" class="section level4">
<h4>Carregando os dados:</h4>
<pre class="r"><code>dados &lt;- load(&quot;C:/Users/tarss/Desktop/SER/CURO_SER/data_041218_1138.RData&quot;)

dados &lt;- features_atual %&gt;% 
  group_by(categoria) %&gt;% 
  dplyr::filter(categoria %in% c(&quot;Torneira Interna&quot;, &quot;Bacia&quot;)) %&gt;% 
  dplyr::sample_n(1000)

dados$categoria &lt;- forcats::fct_drop(dados$categoria)


x &lt;- dados[,c(2:9)]</code></pre>
</div>
<div id="realizaremos-primeiro-o-pca-atraves-do-passo-a-passo-a-seguir" class="section level4">
<h4>Realizaremos, primeiro, o PCA através do passo-a-passo a seguir:</h4>
<ol style="list-style-type: decimal">
<li>Escalonando os dados:</li>
</ol>
<pre class="r"><code>x1 &lt;- scale(x)</code></pre>
<ol start="2" style="list-style-type: decimal">
<li>Cálculo da covariância:</li>
</ol>
<pre class="r"><code>covariancia &lt;- cov(x1)

covariancia</code></pre>
<pre><code>##           duracao      nmoda    volume   inercia       moda      media
## duracao 1.0000000 0.91689273 0.7791928 0.1844582 0.14691699 0.13390061
## nmoda   0.9168927 1.00000000 0.6616528 0.2795798 0.05257201 0.04397017
## volume  0.7791928 0.66165281 1.0000000 0.3465013 0.58379483 0.58795810
## inercia 0.1844582 0.27957983 0.3465013 1.0000000 0.31927738 0.31333773
## moda    0.1469170 0.05257201 0.5837948 0.3192774 1.00000000 0.91005893
## media   0.1339006 0.04397017 0.5879581 0.3133377 0.91005893 1.00000000
## pico    0.2394505 0.11012416 0.6175658 0.1794066 0.79166980 0.89269960
## mediana 0.1385520 0.05113187 0.5963344 0.3487025 0.93681332 0.97219833
##              pico    mediana
## duracao 0.2394505 0.13855199
## nmoda   0.1101242 0.05113187
## volume  0.6175658 0.59633445
## inercia 0.1794066 0.34870252
## moda    0.7916698 0.93681332
## media   0.8926996 0.97219833
## pico    1.0000000 0.84891549
## mediana 0.8489155 1.00000000</code></pre>
<ol start="3" style="list-style-type: decimal">
<li>Autovetores e valores:</li>
</ol>
<pre class="r"><code>auto &lt;- eigen(covariancia)

auto$vectors # Autovetores</code></pre>
<pre><code>##            [,1]        [,2]         [,3]        [,4]        [,5]
## [1,] -0.2262714  0.57486252  0.182626790 -0.01444230 -0.01042984
## [2,] -0.1818174  0.60446964  0.009277947 -0.06133980  0.58649043
## [3,] -0.3946588  0.30214115  0.103746959 -0.04859994 -0.76549174
## [4,] -0.2069312  0.09753486 -0.947487132  0.18197385 -0.05017738
## [5,] -0.4187341 -0.23012777  0.004483958 -0.58790499  0.07365775
## [6,] -0.4307294 -0.24855499  0.039462051  0.04803771  0.18721715
## [7,] -0.4118067 -0.17364071  0.237440717  0.75484338  0.09284042
## [8,] -0.4321073 -0.24178036 -0.010517846 -0.20693956  0.13538739
##             [,6]        [,7]        [,8]
## [1,] -0.20339868  0.73566649 -0.04569684
## [2,]  0.12815923 -0.48565845  0.03808255
## [3,]  0.18950564 -0.34327629 -0.01005750
## [4,] -0.09551898  0.07111807 -0.01098403
## [5,] -0.62097866 -0.13997817 -0.12416400
## [6,]  0.46576307  0.14142691 -0.69053652
## [7,] -0.36022317 -0.12454968  0.14214361
## [8,]  0.40602643  0.21195804  0.69554117</code></pre>
<pre class="r"><code>auto$values # Autovalores</code></pre>
<pre><code>## [1] 4.49166127 2.15763727 0.86811768 0.19745318 0.14605513 0.07234296
## [7] 0.04611518 0.02061732</code></pre>
</div>
<div id="exploraremos-agora-brevemente-alguns-opcoes-de-projecao-do-pca" class="section level4">
<h4>Exploraremos agora, brevemente, alguns opções de projeção do PCA:</h4>
<p>Usaremos para isto o pacote <code>factoextra</code>. Primeiramente, vamos construir nosso objeto <code>pca</code></p>
<pre class="r"><code>x_pca &lt;- prcomp(x, scale = TRUE)

summary(x_pca)</code></pre>
<pre><code>## Importance of components:
##                           PC1    PC2    PC3     PC4     PC5     PC6
## Standard deviation     2.1194 1.4689 0.9317 0.44436 0.38217 0.26897
## Proportion of Variance 0.5615 0.2697 0.1085 0.02468 0.01826 0.00904
## Cumulative Proportion  0.5615 0.8312 0.9397 0.96436 0.98262 0.99166
##                            PC7     PC8
## Standard deviation     0.21474 0.14359
## Proportion of Variance 0.00576 0.00258
## Cumulative Proportion  0.99742 1.00000</code></pre>
<pre class="r"><code>x_pca$rotation</code></pre>
<pre><code>##                PC1         PC2          PC3         PC4         PC5
## duracao -0.2262714  0.57486252 -0.182626790 -0.01444230  0.01042984
## nmoda   -0.1818174  0.60446964 -0.009277947 -0.06133980 -0.58649043
## volume  -0.3946588  0.30214115 -0.103746959 -0.04859994  0.76549174
## inercia -0.2069312  0.09753486  0.947487132  0.18197385  0.05017738
## moda    -0.4187341 -0.23012777 -0.004483958 -0.58790499 -0.07365775
## media   -0.4307294 -0.24855499 -0.039462051  0.04803771 -0.18721715
## pico    -0.4118067 -0.17364071 -0.237440717  0.75484338 -0.09284042
## mediana -0.4321073 -0.24178036  0.010517846 -0.20693956 -0.13538739
##                 PC6         PC7         PC8
## duracao  0.20339868 -0.73566649 -0.04569684
## nmoda   -0.12815923  0.48565845  0.03808255
## volume  -0.18950564  0.34327629 -0.01005750
## inercia  0.09551898 -0.07111807 -0.01098403
## moda     0.62097866  0.13997817 -0.12416400
## media   -0.46576307 -0.14142691 -0.69053652
## pico     0.36022317  0.12454968  0.14214361
## mediana -0.40602643 -0.21195804  0.69554117</code></pre>
<p>É interessante, em momento de exercicio, comparar o obtido através do algoritmo e o resultado da função <code>prcomp</code>.</p>
<p>É possível perceber, também, que com as três componentes principais podemos explicar cerca de 90% da variância dos dados, reduzindo de forma significativa o numéro de dimensões do nosso problema. Vamos, enfim, explorar a projeção das nossas componentes principais.</p>
<pre class="r"><code>factoextra::fviz_pca_biplot(x_pca, repel = FALSE)</code></pre>
<p><img src="/post/2019-05-21-introdução-ao-pc-parte-1_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>Com o gráfico acima verificamos a direção de alguma das variáveis e como os dados se comportam após em função dos dois primeiros componentes gerados pela PCA. Observamos também que duração e nmoda estão bem correlacionados, assim como moda media, pico e mediana estão entre sí.</p>
<p>É pertinente para avaliarmos, também, se a relação entre as variáveis, tendo como base o fenômeno real, se mantem nesta projeção criada.</p>
<p>Podemos, ver também como acontece a projeção, tendo em vista as classificações dos usos domésticos de água:</p>
<pre class="r"><code>factoextra::fviz_pca_ind(x_pca,
             label = &quot;none&quot;, # hide individual labels
             habillage = dados$categoria, # color by groups
             addEllipses = TRUE # Concentration ellipses
             )</code></pre>
<p><img src="/post/2019-05-21-introdução-ao-pc-parte-1_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>Ao observar, este plot, pode-se perceber que há uma certa definição dos grupos, sendo possível, com métodos como o <code>knn</code> treinar um modelo para determinar qual a categoria de uma nova observação a partir das componentes principais, nas quais a correlação entre as variáveis tende a ser nula.</p>
<p>Por fim, o PCA também pode ser utilizado para indicar possíveis <code>outliers</code> através do <code>Q contribution</code>. Deve-se, neste momento, realizar um pequeno esforço de abstração para entender que, num caso multivariado, há dois principais tipos de outliers. O primeiro diz respeito a valores aberrantes das variáveis, fora do range considerado aceitável para esta.</p>
<p>O segundo, e mais interessante no caso do PCA, versa sobre a quebra da correlação, ou seja, mesmo que os dados estejam dentro dos limites aceitáveis, uma dada observação nao respeita a correlação existente entre as variáveis.</p>
<pre class="r"><code>p &lt;- fviz_pca_ind(x_pca, addEllipses = TRUE, habillage = dados$categoria, ellipse.level=0.95, geom =                           c(&quot;text&quot;,&quot;point&quot;),
                  repel = FALSE) + xlim(-10, 15) + ylim (-10, 10) + labs(title =&quot;PCA&quot;, x = &quot;PC1&quot;, y = &quot;PC2&quot;)
p</code></pre>
<p><img src="/post/2019-05-21-introdução-ao-pc-parte-1_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>Observando a elipse formada, inferimos a cerca dos <code>outliers</code>. Para selecionar e remover estes valores ditos aberrantes vamos utilizar as seguintes funções:</p>
<pre class="r"><code>outlier &lt;- which(p$data$contrib &gt; .5)  #podemos utilizar maiores ou menores valores para o Q.

outlier</code></pre>
<pre><code>## [1]   11   57  101  246  260 1278 1605 1974 1979</code></pre>
<pre class="r"><code>#Removendo os valores: 

data_pca &lt;- x_pca$x %&gt;% 
  as.tibble() %&gt;% 
  slice(-outlier)</code></pre>
<p>Em publicação posterior veremos como funciona o PCA em texto!</p>
</div>
</div>
</div>
